"""
Main script for refolding pipeline on unconditional generation.
The refolding pipeline use ESMFold as default.

To run AlphaFold2: 
> python scaffold_lab/unconditional/refolding.py inference.predict_method='AlphaFold2'

To run ESMFold and AlphaFold2 simultaneously:
> python scaffold_lab/unconditional/refolding.py inference.predict_method='[AlphaFold2, ESMFold]'

"""

import os
import tree
import time
import numpy as np
import hydra
import torch
import subprocess
import logging
import warnings
import pandas as pd
import sys
import rootutils
import shutil
import GPUtil
from pathlib import Path
from typing import Optional, Dict, Union, List
from omegaconf import DictConfig, OmegaConf
from collections import defaultdict

import esm
import biotite.structure.io as strucio
from biotite.sequence.io import fasta


path = rootutils.find_root(search_from='./', indicator=[".git", "setup.cfg"])
rootutils.set_root(
    path=path, # path to the root directory
    project_root_env_var=True, # set the PROJECT_ROOT environment variable to root directory
    dotenv=True, # load environment variables from .env if exists in root directory
    pythonpath=True, # add root directory to the PYTHONPATH (helps with imports)
    cwd=True, # change current working directory to the root directory (helps with filepaths)
)

import scaffold_lab
from analysis import utils as au
from data import structure_utils as su
from analysis import diversity as du
from analysis import novelty as nu
from analysis import plot as pu


class UnconditionalRefolder:

    """
    Perform refolding analysis on a set of protein backbones.
    Organized by the following steps:
    1. Config initialization
    2. Run ProteinMPNN on a given set of PDB files
    3. Run ESMFold / AlphaFold2 on sequences generated by ProteinMPNN ()
    4. Calculate the metrics (RMSD, TM-score, pLDDT, etc.) and write information into a csv file.
    
    One can also modify this script to perform fixed backbone design and evaluations on refoldability.
    Adapted from https://github.com/jasonkyuyim/se3_diffusion/blob/master/experiments/inference_se3_diffusion.py
    """
    
    def __init__(
        self,
        conf:DictConfig,
        conf_overrides: Dict=None
        ):
        
        self._log = logging.getLogger(self.__class__.__name__)
        warnings.filterwarnings("ignore", module="biotite.*|psutil.*|matplotlib.*", category=UserWarning)
        
        OmegaConf.set_struct(conf, False)
        
        self._package_dir = "/".join(scaffold_lab.__path__._path[0].split("/")[:-1])
        self._conf = conf
        self._infer_conf = conf.inference
        self._sample_conf = self._infer_conf.samples

        # Sanity check
        if self._sample_conf.seq_per_sample < self._sample_conf.mpnn_batch_size:
            raise ValueError(f'Sequences per sample {self._sample_conf.seq_per_sample} < \
            batch size {self._sample_conf.mpnn_batch_size}!')
        
        self._rng = np.random.default_rng(self._infer_conf.seed)
        
        # Set-up accelerator
        if torch.cuda.is_available():
            if self._infer_conf.gpu_id is None:
                available_gpus = ''.join(
                    [str(x) for x in GPUtil.getAvailable(
                        order="memory", limit = 8)]
                )
                self.device = f'cuda:{available_gpus[0]}'
            else:
                self.device = f'cuda:{self._infer_conf.gpu_id}'
        else:
            self.device = 'cpu'
        self._log.info(f'Using device: {self.device}')

        # Customizing different structure prediction methods 
        self._forward_folding = self._infer_conf.predict_method
        if 'AlphaFold2' in self._forward_folding:
            self._af2_conf = self._infer_conf.af2
            colabfold_path = self._af2_conf.executive_colabfold_path
            current_path = os.environ.get('PATH', '')
            os.environ['PATH'] = colabfold_path + ":" + current_path
            if self.device == 'cpu':
                self._log.info(f"You're running AlphaFold2 on {self.device}.")
        
        # Set-up directories
        output_dir = self._infer_conf.output_dir
        
        self._output_dir = output_dir
        os.makedirs(self._output_dir, exist_ok=True)
        self._pmpnn_dir = os.path.join(self._package_dir, self._infer_conf.pmpnn_dir)
        self._sample_dir = self._infer_conf.backbone_pdb_dir
        self._CA_only = self._infer_conf.CA_only
        self._hide_GPU_from_pmpnn = self._infer_conf.hide_GPU_from_pmpnn
        
        # Save config
        config_folder = os.path.basename(Path(self._output_dir))
        config_path = os.path.join(self._output_dir, f"{config_folder}.yaml")
        with open(config_path, 'w') as f:
            OmegaConf.save(config=self._conf, f=f)
        self._log.info(f'Saving self-consistency config to {config_path}')
        
        # Load models and experiment
        if 'cuda' in self.device:
            self._folding_model = esm.pretrained.esmfold_v1().eval()
        elif self.device == 'cpu': # ESMFold is not supported for half-precision model when running on CPU
            self._folding_model = esm.pretrained.esmfold_v1().float().eval()
        self._folding_model = self._folding_model.to(self.device)
    
    def run_sampling(self):
        
        # Run ProteinMPNN
        for pdb_file in os.listdir(self._sample_dir):

            naming_number = 1
            if ".pdb" not in pdb_file:
                continue

            # Backbone name handling
            all_name = os.path.splitext(pdb_file)[0]

            backbone_name = os.path.splitext(pdb_file)[0]
            print(f'sample_dir: {self._sample_dir}')
            #basename_dir = os.path.basename(os.path.normpath(self._sample_dir))
            #backbone_dir = os.path.join(self._output_dir, basename_dir, f'{backbone_name}')
            # Save outputs
            backbone_dir = os.path.join(self._output_dir, backbone_name)
            if "ESMFold" in self._forward_folding:
                summary_fn = os.path.join(backbone_dir, 'self_consistency/esm_eval_results.csv')
            else:
                assert 'AlphaFold2' in self._forward_folding
                summary_fn = os.path.join(backbone_dir, 'self_consistency/af2_eval_results.csv')
            if os.path.exists(summary_fn):
                self._log.warning(f'Backbone {backbone_name} results already exists. Continuing...')

                continue
            
            os.makedirs(backbone_dir, exist_ok=True)
            self._log.info(f'Running self-consistency on {backbone_name}')
            print(f'pdb_file:{pdb_file}')
            print(f'backbone_dir:{backbone_dir}')
            shutil.copy2(os.path.join(self._sample_dir, pdb_file), backbone_dir)
            self._log.info(f'copied {pdb_file} to {backbone_dir}')
            
            #seperate_pdb_folder = os.path.join(backbone_dir, backbone_name)
            pdb_path = os.path.join(backbone_dir, pdb_file)
            sc_output_dir = os.path.join(backbone_dir, 'self_consistency')
            os.makedirs(sc_output_dir, exist_ok=True)
            shutil.copy(pdb_path, os.path.join(
                sc_output_dir, os.path.basename(pdb_path)))

            _ = self.run_self_consistency(
                decoy_pdb_dir=sc_output_dir,
                reference_pdb_path=pdb_path,
                motif_mask=None
            )
            self._log.info(f'Done sample: {pdb_path}')
    
    def run_self_consistency(
            self,
            decoy_pdb_dir: str,
            reference_pdb_path: str,
            motif_mask: Optional[np.ndarray]=None):
        """Run self-consistency on design proteins against reference protein.
        
        Args:
            decoy_pdb_dir: directory where designed protein files are stored.
            reference_pdb_path: path to reference protein file
            motif_mask: Optional mask of which residues are the motif.

        Returns:
            Writes ProteinMPNN outputs to decoy_pdb_dir/seqs
            Writes ESMFold outputs to decoy_pdb_dir/esmf
            Writes results in decoy_pdb_dir/sc_results.csv
        """

        # Run ProteinMPNN
        
        jsonl_path = os.path.join(decoy_pdb_dir, "parsed_pdbs.jsonl")
        process = subprocess.Popen([
            'python',
            f'{self._pmpnn_dir}/helper_scripts/parse_multiple_chains.py',
            f'--input_path={decoy_pdb_dir}',
            f'--output_path={jsonl_path}',
        ])
        
        _ = process.wait()
        num_tries = 0
        ret = -1
        pmpnn_args = [
            sys.executable,
            f'{self._pmpnn_dir}/protein_mpnn_run.py',
            '--out_folder',
            decoy_pdb_dir,
            '--jsonl_path',
            jsonl_path,
            '--num_seq_per_target',
            str(self._sample_conf.seq_per_sample),
            '--sampling_temp',
            '0.1',
            '--seed',
            '33',
            '--batch_size',
            str(self._sample_conf.mpnn_batch_size),
        ]
        self._log.info(f'Running ProteinMPNN with command {" ".join(pmpnn_args)}')
        if self._infer_conf.gpu_id is not None:
            pmpnn_args.append('--device')
            pmpnn_args.append(str(self._infer_conf.gpu_id))
        if self._CA_only == True:
            pmpnn_args.append('--ca_only')
        
        while ret < 0:
            try:
                print("Running ProteinMPNN...")
                # Setting CUDA_VISIBLE_DEVICES to an empty string to hide all GPUs
                env = os.environ.copy()
                if self._hide_GPU_from_pmpnn:
                    env["CUDA_VISIBLE_DEVICES"] = ""
                process = subprocess.Popen(
                    pmpnn_args,
                    env=env,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.STDOUT
                )
                ret = process.wait()
            except Exception as e:
                num_tries += 1
                self._log.info(f'Failed ProteinMPNN. Attempt {num_tries}/5')
                torch.cuda.empty_cache()
                if num_tries > 4:
                    raise e
        mpnn_fasta_path = os.path.join(
            decoy_pdb_dir,
            'seqs',
            os.path.basename(reference_pdb_path).replace('.pdb', '.fa')
        )

        # Run ESMFold on each ProteinMPNN sequence and calculate metrics.
        mpnn_results = {
            'tm_score': [],
            'sample_path': [],
            'header': [],
            'sequence': [],
            'rmsd': [],
            'pae': [],
            'ptm': [],
            'plddt': [],
            'length': [],
            'mpnn_score': [],
            'sample_idx': []
        }
        if motif_mask is not None:
            # Only calculate motif RMSD if mask is specified.
            mpnn_results['refold_motif_rmsd'] = []
        esmf_dir = os.path.join(decoy_pdb_dir, 'esmf')
        af2_raw_dir = os.path.join(decoy_pdb_dir, 'af2_raw_outputs')
        fasta_seqs = fasta.FastaFile.read(mpnn_fasta_path)
        filtered_seqs = {header: seq for header, seq in fasta_seqs.items() if header.startswith("T=0")} # Drop original sequence
        if self._sample_conf.sort_by_score:
        # Only take seqs with lowerst global score to enter refolding
            scores = []
            for i, (header, string) in enumerate(filtered_seqs.items()):
                #if i == 0:
                #    global_score = float(header.split(", ")[2].split("=")[1])
                #    original_seq = (global_score, header, string)
                #else:
                global_score = float(header.split(", ")[3].split("=")[1])
                scores.append((global_score, header, string))
            scores.sort(key=lambda x: x[0])

            top_seqs_list = scores[:self._sample_conf.seq_per_sample]
            #top_seqs_list.insert(0, original_seq) # Include the original seq
            top_seqs = {header: seq for _, header, seq in top_seqs_list}

            top_seqs_path = os.path.join(
                decoy_pdb_dir,
                'seqs',
                f'top_score_{os.path.basename(reference_pdb_path)}'.replace('.pdb', '.fa')
            )
            print(f'top seqs: {top_seqs}\ntype: {type(top_seqs)}\n')
            _ = au.write_seqs_to_fasta(top_seqs, top_seqs_path)
        else:
            filtered_seqs = {header: seq for header, seq in fasta_seqs.items() if header.startswith("T=0")}
            #print(f'filtered_seqs: {filtered_seqs}')
            _ = au.write_seqs_to_fasta(filtered_seqs, mpnn_fasta_path)
        
        seqs_to_refold = top_seqs_path if self._sample_conf.sort_by_score else mpnn_fasta_path
        seqs_dict = fasta.FastaFile.read(seqs_to_refold)
        

        sample_feats = su.parse_pdb_feats('sample', reference_pdb_path)

        if 'ESMFold' in self._forward_folding:
            os.makedirs(esmf_dir, exist_ok=True)
            for i, (header, string) in enumerate(seqs_dict.items()):
                
                # Get score for ProteinMPNN
                if header.startswith("T=0"):
                    idx = header.split('sample=')[1].split(',')[0]
                    score = float(header.split(", ")[3].split("=")[1])
                else:
                    idx = 0
                    score = float(header.split(", ")[2].split("=")[1])
                # Run ESMFold
                self._log.info(f'Running ESMFold......')
                esmf_sample_path = os.path.join(esmf_dir, f'sample_{idx}.pdb')
                _, full_output = self.run_esmfold(string, esmf_sample_path)
                esmf_feats = su.parse_pdb_feats('folded_sample', esmf_sample_path)
                sample_seq = su.aatype_to_seq(sample_feats['aatype'])

                # Calculate scTM of ESMFold outputs with reference protein
                _, tm_score = su.calc_tm_score(
                    sample_feats['bb_positions'], esmf_feats['bb_positions'],
                    sample_seq, sample_seq)
                rmsd = su.calc_aligned_rmsd(
                    sample_feats['bb_positions'], esmf_feats['bb_positions'])
                pae = torch.mean(full_output['predicted_aligned_error']).item()
                ptm = full_output['ptm'].item()
                plddt = full_output['mean_plddt'].item()
                if motif_mask is not None:
                    sample_motif = sample_feats['bb_positions'][motif_mask]
                    esm_motif = esmf_feats['bb_positions'][motif_mask]
                    refold_motif_rmsd = su.calc_aligned_rmsd(
                        sample_motif, esm_motif)
                    mpnn_results['refold_motif_rmsd'].append(f'{refold_motif_rmsd:.3f}')
                mpnn_results['sample_idx'].append(int(idx))
                mpnn_results['rmsd'].append(f'{rmsd:.3f}')
                mpnn_results['tm_score'].append(f'{tm_score:.3f}')
                mpnn_results['sample_path'].append(os.path.abspath(esmf_sample_path))
                mpnn_results['header'].append(header)
                mpnn_results['sequence'].append(string)
                mpnn_results['pae'].append(f'{pae:.3f}')
                mpnn_results['ptm'].append(f'{ptm:.3f}')
                mpnn_results['plddt'].append(f'{plddt:.3f}')
                mpnn_results['length'].append(len(string))
                mpnn_results['mpnn_score'].append(f'{score:.3f}')

            # Save results to CSV
            esm_csv_path = os.path.join(decoy_pdb_dir, 'esm_eval_results.csv')
            mpnn_results = pd.DataFrame(mpnn_results)
            mpnn_results.sort_values('sample_idx', inplace=True)
            mpnn_results.to_csv(esm_csv_path, index=False)

        # Run AlphaFold2 (No MSA)
        if 'AlphaFold2' in self._forward_folding:
            self._log.info(f'Running AlphaFold2......')

            _ = self.run_af2(seqs_to_refold, af2_raw_dir)
            af2_dir = os.path.join(decoy_pdb_dir, 'af2')
            os.makedirs(af2_dir, exist_ok=True)
            af2_outputs = au.cleanup_af2_outputs(
                raw_dir=af2_raw_dir,
                clean_dir=os.path.join(decoy_pdb_dir, 'af2'),
                remove_after_cleanup=self._af2_conf.remove_raw_outputs
            )

            for i, (header, string) in enumerate(seqs_dict.items()):
                # Find index and score
                if header.startswith("T=0"):
                    idx = header.split('sample=')[1].split(',')[0]
                    score = float(header.split(", ")[3].split("=")[1])
                else:
                    idx = 0
                    score = float(header.split(", ")[2].split("=")[1])

                af2_sample_path = os.path.join(af2_dir, f'sample_{idx}.pdb')
                af2_feats = su.parse_pdb_feats('folded_sample', af2_sample_path)
                sample_seq = su.aatype_to_seq(sample_feats['aatype'])

                # Calculation
                _, tm_score = su.calc_tm_score(
                    sample_feats['bb_positions'], af2_feats['bb_positions'],
                    sample_seq, sample_seq)
                rmsd = su.calc_aligned_rmsd(
                    sample_feats['bb_positions'], af2_feats['bb_positions'])
                if motif_mask is not None:
                    sample_motif = sample_feats['bb_positions'][motif_mask]
                    af2_motif = af2_feats['bb_positions'][motif_mask]
                    refold_motif_rmsd = su.calc_aligned_rmsd(
                        sample_motif, af2_motif)
                    af2_outputs[f'sample_{idx}']['refold_motif_rmsd'] = f'{refold_motif_rmsd:.3f}'
                af2_outputs[f'sample_{idx}']['rmsd'] = f'{rmsd:.3f}'
                af2_outputs[f'sample_{idx}']['tm_score'] = f'{tm_score:.3f}'
                af2_outputs[f'sample_{idx}']['header'] = header
                af2_outputs[f'sample_{idx}']['sequence'] = string
                af2_outputs[f'sample_{idx}']['length'] = len(string)
                af2_outputs[f'sample_{idx}']['mpnn_score'] = f'{score:.3f}'
                af2_outputs[f'sample_{idx}']['sample_idx'] = int(idx)
            print(f'final_outputs: {af2_outputs}')
            af2_csv_path = os.path.join(decoy_pdb_dir, 'af2_eval_results.csv')
            af2_df = pd.DataFrame.from_dict(af2_outputs, orient='index')
            af2_df.reset_index(inplace=True)
            af2_df.rename(columns={'index': 'sample'}, inplace=True)
            af2_df.drop('sample', axis=1, inplace=True)
            af2_df.sort_values('sample_idx', inplace=True)
            af2_df.to_csv(af2_csv_path, index=False)

        if 'ESMFold' in self._forward_folding and 'AlphaFold2' in self._forward_folding:
            esm_results = pd.read_csv(esm_csv_path)
            af2_results = pd.read_csv(af2_csv_path)
            esm_results['folding_method'] = 'ESMFold'
            af2_results['folding_method'] = 'AlphaFold2'
            joint_results = pd.concat([esm_results, af2_results], ignore_index=True)
            joint_results.to_csv(os.path.join(decoy_pdb_dir, 'joint_eval_results.csv'), index=False)


    def run_esmfold(self, sequence: str, save_path: Union[str, Path]):
        """
        Run ESMFold on sequence.
        TBD: Add options for OmegaFold and AlphaFold2.
        """
        with torch.no_grad():
            output = self._folding_model.infer(sequence)
            output_dict = {key: value.cpu() for key, value in output.items()}
            output = self._folding_model.output_to_pdb(output)
        with open(save_path, "w") as f:
            f.write(output[0])
        return output, output_dict  

    def run_af2(self, sequence: str, save_path: Union[str, Path]):
        """
        Run AlphaFold2 (single-sequence) through LocalColabFold.
        """

        #_ = process.wait()
        num_tries_af2 = 0
        ret_af2 = -1

        # Setting AF2 args
        af2_args = [
            #sys.executable,
            'colabfold_batch',
            sequence,
            save_path,
            '--msa-mode',
            'single_sequence',
            '--num-recycle',
            str(self._af2_conf.recycle),
            '--random-seed',
            str(self._af2_conf.seed),
            '--model-type',
            self._af2_conf.model_type,
            '--num-models',
            str(self._af2_conf.num_models),
        ]
        if self._af2_conf.num_models > 1:
            af2_args.append('--rank')
            af2_args.append(self._af2_conf.rank)
        if self._af2_conf.use_amber_relax:
            af2_args.append('--amber')
            af2_args.append('--num-relax')
            af2_args.append(str(self._af2_conf.num_relax))
            if self._af2_conf.use_gpu_relax:
                af2_args.append('--use-gpu-relax')

        # Run AF2
        while ret_af2 < 0:
            try:
                process_af2 = subprocess.Popen(
                    af2_args,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.STDOUT
                )
                
                ret_af2 = process_af2.wait()
            except Exception as e:
                num_tries_af2 += 1
                self._log.info(f'Hmm...Maybe some error occurs during executing AlphaFold2. Tried {num_tries_af2}/5')
                torch.cuda.empty_cache()
                if num_tries_af2 > 10:
                    raise e


class UnconditionalEvaluator:
    
    def __init__(
        self, 
        conf:DictConfig,
        conf_overrides: Dict=None
        ):

        self._log = logging.getLogger(self.__class__.__name__)
        logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)

        OmegaConf.set_struct(conf, False)

        self._conf = conf
        self._infer_conf = conf.inference
        self._eval_conf = conf.evaluation
        self._result_dir = self._infer_conf.output_dir

        self._foldseek_path = self._eval_conf.foldseek_path
        self._foldseek_database = self._eval_conf.foldseek_database
        self._package_dir = "/".join(scaffold_lab.__path__._path[0].split("/")[:-1])
        self._assist_protein_path = os.path.join(self._package_dir, self._eval_conf.assist_protein)
        self._tm_threshold = self._eval_conf.tmscore_threshold
        self._visualize = self._eval_conf.visualize

        self.folding_method = self._infer_conf.predict_method

        self._rng = np.random.default_rng(self._infer_conf.seed)

        # Hardware resources
        if self._eval_conf.foldseek_cores_for_pdbTM != "None":
            self._num_cpu_cores = self._eval_conf.foldseek_cores_for_pdbTM
            self._log.info(f"""You're using {self._num_cpu_cores} for Foldseek-search.
            You can set this value to `None` in configuration if you want to try maximizing your CPU utility
            to speed up evaluation process.""")
        else:
            self._num_cpu_cores = os.cpu_count()
            self._log.info(f"""Detecting {self._num_cpu_cores}, trying to maximize CPU utility for Foldseek-search.
            If you're using multiple GPUs and detecting slow evaluation process,
            it is recommend to set `evaluation.foldseek_cores.for_pdbTM` in configuration.
            """)

        # Merge results into one csv file
        if 'ESMFold' in self.folding_method and 'AlphaFold2' in self.folding_method:
            self.prefix = 'joint'
        elif 'ESMFold' in self.folding_method and 'AlphaFold2' not in self.folding_method:
            self.prefix = 'esm'
        else:
            self.prefix = 'af2'


    def _process_results(self, prefix: str):
        """Process results for a single forward folding method (ESMFold / AF2)."""
        results_df, pdb_count = au.csv_merge(root_dir=self._result_dir, prefix=prefix)

        # Analyze outputs
        complete_results, summary_results, designability_count, backbones, best_contender = au.analyze_success_rate_uncond(
            merged_data=results_df, group_mode="all", prefix=prefix
        )

        summary_csv_path = os.path.join(self._result_dir, f"{prefix}_summary_results.csv")
        complete_csv_path = os.path.join(self._result_dir, f"{prefix}_complete_results.csv")
        complete_results.to_csv(complete_csv_path, index=False)
        summary_column_order = [
            "sample_idx",
            "Success",
            "rmsd",
            "length",
            "sequence",
            "sample_path",
            "backbone_path",
        ]
        summary_results.to_csv(summary_csv_path, columns=summary_column_order, index=False)

        # Auxiliary metrics
        if not best_contender is None:
            best_scaffold_dir = os.path.join(self._result_dir, f"{prefix}_best_contender")
            os.makedirs(best_scaffold_dir, exist_ok=True)
            
            best_contender_csv_path = os.path.join(best_scaffold_dir, f"{prefix}_best_contender.csv")
            best_contender.to_csv(best_contender_csv_path, index=False)
            
            best_contender_path = best_contender['backbone_path'].iloc[0]
            shutil.copy(best_contender_path, os.path.join(best_scaffold_dir, os.path.basename(best_contender_path)))

        return complete_results, backbones, designability_count, pdb_count, best_contender


    def _evaluate_diversity(
        self, 
        backbones: set, 
        prefix: str
        ):
        """Run diversity evaluation with Foldseek-Cluster."""
        successful_backbone_dir = os.path.join(self._result_dir, f"{prefix}_successful_backbones")
        os.makedirs(successful_backbone_dir, exist_ok=True)

        for pdb in backbones:
            shutil.copy(pdb, os.path.join(successful_backbone_dir, os.path.basename(pdb)))

        diversity = du.foldseek_cluster(
            input=successful_backbone_dir,
            assist_protein_path=self._assist_protein_path,
            tmscore_threshold=self._tm_threshold,
            alignment_type=1,
            output_mode="DICT",
            save_tmp=True,
            foldseek_path=self._foldseek_path,
        )
        self._log.info(
            f"Diversity Calculation for {prefix} finished.\t"
            f"Designable scaffolds: {diversity['Samples']}\t"
            f"Unique solutions: {diversity['Clusters']}"
        )

        # Create unique designable backbone directory
        diversity_result_path = os.path.join(successful_backbone_dir, "diversity_cluster.tsv")
        unique_backbones_dir = os.path.join(self._result_dir, f"{prefix}_unique_designable_backbones")
        os.makedirs(unique_backbones_dir, exist_ok=True)

        cluster_centers = set()
        cluster_dict = {}

        if os.path.exists(diversity_result_path):
            with open(diversity_result_path, "r") as f:
                cluster_info = f.readlines()

            cluster_map = defaultdict(list)
            for line in cluster_info:
                center, member = line.strip().split("\t")
                cluster_map[center].append(member)
            cluster_map.pop("assist_protein.pdb", None)

            for idx, (center, members) in enumerate(cluster_map.items(), start=1):
                cluster_centers.add(center)
                cluster_dict[str(idx)] = {"center": center, "member": members}

            if "assist_protein.pdb" in cluster_centers:
                cluster_info.remove("assist_protein.pdb")

            for pdb in cluster_centers:
                old_path = os.path.join(successful_backbone_dir, pdb)
                shutil.copy(old_path, unique_backbones_dir)
        else:
            self._log.info(
                f"Diversity results for {prefix} not found. Please check if Foldseek clustered properly or no designable backbones are present."
            )

        return diversity, successful_backbone_dir, unique_backbones_dir, cluster_dict


    def _evaluate_novelty(
        self, 
        complete_results: Union[str, Path, pd.DataFrame], 
        successful_backbone_dir: Union[str, Path], 
        unique_backbones_dir: Union[str, Path],
        clusters: Dict,
        prefix: str = "esm"
        ):
        """Run novelty evaluation with Foldseek-Search."""
        success_results = complete_results[complete_results["Success"] == True]
        novelty_csv_path = os.path.join(self._result_dir, f"{prefix}_novelty_results.csv")
        if os.listdir(successful_backbone_dir):
            if not os.path.exists(novelty_csv_path): 
                results_with_novelty = nu.calculate_novelty(
                    input_csv=success_results,
                    foldseek_database_path=self._eval_conf.foldseek_database,
                    max_workers=self._num_cpu_cores,
                    cpu_threshold=75.0,
                )
            else:
                results_with_novelty = pd.read_csv(novelty_csv_path)
                self._log.info(f"Novelty results already exist. Continuing...")

            unique_backbones = results_with_novelty[results_with_novelty["sample_idx"] == 1]
            novelty_lookup = dict(zip(unique_backbones["backbone_path"].apply(os.path.basename), unique_backbones["pdbTM"]))

            for cluster_id, cluster_info in clusters.items():
                center = cluster_info["center"]
                members = cluster_info["member"]

                cluster_novelty_values = [novelty_lookup.get(member, None) for member in members]
                cluster_novelty_values = [val for val in cluster_novelty_values if val is not None]

                if cluster_novelty_values:
                    mean_novelty = sum(cluster_novelty_values) / len(cluster_novelty_values)
                    clusters[cluster_id]["mean_novelty"] = mean_novelty
                else:
                    clusters[cluster_id]["mean_novelty"] = None

            # Weighted novelty across clusters
            novelty_values = [cluster_info["mean_novelty"] for cluster_info in clusters.values()]
            weighted_novelty = sum(novelty_values) / len(novelty_values)

            if len(os.listdir(unique_backbones_dir)) != len(novelty_values):
                self._log.warning(f"""
            Incompatible numbers of clusters{len(os.listdir(unique_backbones_dir))} 
            and number of novelty results {len(novelty_values)}, please have a check!
            """)

            novelty_score = 1 - weighted_novelty
            max_novelty = results_with_novelty["pdbTM"].min()
            self._log.info(
                f"Novelty Calculation for {prefix} finished.\n"
                f"Novelty score (1 - pdbTM) among successful backbones weighted by number of clusters: {novelty_score:.3f}\n"
                f"The most novel designable backbone has a pdbTM of {max_novelty:.3f}"
            )
            results_with_novelty.to_csv(novelty_csv_path, index=False)
        else:
            self._log.info(f"No successful backbone was found for {prefix}. Skipping novelty calculation.")
            novelty_score = 0
        return novelty_score


    def run_evaluation(self):
        """Run evaluation for all specified folding methods."""
        diversity_results = {}
        novelty_results = {}
        designability_counts = {}
        pdb_counts = {}

        for method in self.folding_method:
            self._log.info(f"Processing results for folding method: {method}")
            prefix = "esm" if method == "ESMFold" else "af2"

            # Process results and calculate diversity and novelty
            complete_results, backbones, designability_count, pdb_count, best_contender = self._process_results(prefix)
            diversity, successful_backbone_dir, unique_clusters, clusters_information = self._evaluate_diversity(backbones, prefix)
            novelty_score = self._evaluate_novelty(
                complete_results=complete_results,
                successful_backbone_dir=successful_backbone_dir,
                unique_backbones_dir=unique_clusters,
                clusters=clusters_information, 
                prefix=prefix)

            # Collect results
            diversity_results[prefix] = diversity
            novelty_results[prefix] = novelty_score
            designability_counts[prefix] = designability_count
            pdb_counts[prefix] = pdb_count

            # Auxiliary metrics
            au.write_auxiliary_metrics_uncond(
                stored_path=self._result_dir,
                auxiliary_results=best_contender,
                prefix=prefix
            )


        # Write summary outputs
        for prefix in diversity_results.keys():
            au.write_summary_results(
                stored_path=self._result_dir,
                pdb_count=pdb_counts[prefix],
                designable_count=designability_counts[prefix],
                diversity_result=diversity_results[prefix],
                novelty_value=novelty_results[prefix],
                prefix=prefix
            )

        # Optional visualization
        if self._visualize:
            for method in self.folding_method:
                self._log.info(f"Performing visualization for {method}.")
                prefix = "esm" if method == "ESMFold" else "af2"

                pu.unconditional_pymol_write(
                    unique_designable_backbones=os.path.join(self._result_dir, f'{prefix}_unique_designable_backbones'),
                    save_path=os.path.join(self._result_dir, f'{prefix}_pymol_session.pse')
                    )

                # Auxiliary metrics
                best_contender_path = os.path.join(self._result_dir, f'{prefix}_best_contender')
                if os.listdir(best_contender_path):
                    pu.unconditional_pymol_write(
                        unique_designable_backbones=best_contender_path,
                        save_path=os.path.join(best_contender_path, f'{prefix}_best_contender.pse')
                    )
                



@hydra.main(version_base=None, config_path="../../config",
        config_name="unconditional.yaml")
def run(conf: DictConfig) -> None:
    

    # Check that path to foldseek database has been specified
    if not conf.evaluation.get("foldseek_database"):
        raise ValueError("The 'foldseek_database' must be specified in the configuration.")
    # Perform fixed backbone design and forward folding
    print("Starting refolding for unconditional task......")
    start_time = time.time()
    refolder = UnconditionalRefolder(conf)
    refolder.run_sampling()
    elapsed_time = time.time() - start_time
    print(f"Refolding finished in {elapsed_time:.2f}s.")

    # Perform analysis on outputs
    start_time = time.time()
    evaluator = UnconditionalEvaluator(conf)
    evaluator.run_evaluation()
    elapsed_time = time.time() - start_time
    print(f'Evaluation finished in {elapsed_time:.2f}s. Viola!')
    
if __name__ == '__main__':
    run()
